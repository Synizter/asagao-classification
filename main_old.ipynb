{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import normalize\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataset = []\n",
    "msk_dataset = []\n",
    "\n",
    "f_img = sorted(glob('dataset/train/asagao/*'))\n",
    "for f in f_img:\n",
    "    image = cv2.imread(f)\n",
    "    # image = image.resize((SIZE, SIZE))\n",
    "    #Nomalize an image from 0-255 to 0 - 1\n",
    "    image = image.astype('float32') / 255.0\n",
    "    img_dataset.append(np.array(image))\n",
    "\n",
    "f_msk = sorted(glob('dataset/validation/asagao/*'))\n",
    "for f in f_msk:\n",
    "    image = cv2.imread(f, cv2.IMREAD_GRAYSCALE)\n",
    "    (thresh, im_bw) = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    image = cv2.threshold(image, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    # image = Image.fromarray(image)\n",
    "    # image = image.resize((SIZE, SIZE))\n",
    "    msk_dataset.append(np.array(image))\n",
    "asagao_dataset = np.array(img_dataset)\n",
    "asagao_msk_dataset = np.expand_dims(np.array(msk_dataset) /255., -1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "X_train, X_test, y_train, y_test = train_test_split(asagao_dataset, asagao_msk_dataset, test_size= 0.10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train with asagao pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_previous = history\n",
    "history = model.fit(X_train, \n",
    "                    y_train,\n",
    "                    verbose = 1,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 100,\n",
    "                    validation_data = (X_test, y_test),\n",
    "                    shuffle = False)\n",
    "model.save(\"50m_30m_Unet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = history_previous\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, acc, 'b', label='Training acc',linewidth=2)\n",
    "plt.plot(epochs, val_acc, 'r--', label='Validation acc',linewidth=2)\n",
    "plt.title('Training  accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "\n",
    "plt.plot(epochs, val_loss, 'k', label='Validationloss ')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#IOU\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_thresholded = y_pred > 0.6\n",
    "\n",
    "intersection = np.logical_and(y_test, y_pred_thresholded)\n",
    "union = np.logical_or(y_test, y_pred_thresholded)\n",
    "iou_score = np.sum(intersection) / np.sum(union)\n",
    "print(\"IoU socre is: \", iou_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test within known data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tile(tile, sens = 0.001):\n",
    "    tile = tile.astype('float32') / 255.0\n",
    "    test_img_other_input=np.expand_dims(tile, 0)\n",
    "    prob_map = (model.predict(test_img_other_input) > sensitivty).astype(np.uint8)\n",
    "    return prob_map[0]\n",
    "    \n",
    "\n",
    "# file_name = \"asagao_0001.png\"\n",
    "# train_file = 'dataset/train/asagao/' + file_name\n",
    "# test_file = 'dataset/validation/asagao/' + file_name.replace(\"asagao_\", \"asagao_mask_\" )\n",
    "\n",
    "# print(train_file, test_file)\n",
    "# sensitivty = 0.4\n",
    "# test_img_other = cv2.imread(train_file)\n",
    "# r = predict_tile(test_img_other, sensitivty)\n",
    "\n",
    "# plt.figure(figsize=(20,20))\n",
    "# actual = cv2.imread(train_file)\n",
    "# mask = cv2.imread(test_file)\n",
    "# plt.subplot(131),plt.imshow(actual),plt.title(\"Actual\")\n",
    "# plt.subplot(132),plt.imshow(mask),plt.title(\"Actual mask\")\n",
    "# plt.subplot(133),plt.imshow(r, cmap=\"gray\"),plt.title(\"Prediction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with tiled image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('50m_30m_Unet')\n",
    "sensitivity = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found previous tiled images, attemp delete\n",
      "Found previous predicted files, attemp delete\n",
      "predicted dataset/test/test1/Tile_0001.png in 0.06397486 s\n",
      "predicted dataset/test/test1/Tile_0002.png in 0.08188486 s\n",
      "predicted dataset/test/test1/Tile_0003.png in 0.07268691 s\n",
      "predicted dataset/test/test1/Tile_0004.png in 0.07395077 s\n",
      "predicted dataset/test/test1/Tile_0005.png in 0.05598974 s\n",
      "predicted dataset/test/test1/Tile_0006.png in 0.05110860 s\n",
      "predicted dataset/test/test1/Tile_0007.png in 0.09715867 s\n",
      "predicted dataset/test/test1/Tile_0008.png in 0.06625485 s\n",
      "predicted dataset/test/test1/Tile_0009.png in 0.06606221 s\n",
      "predicted dataset/test/test1/Tile_0010.png in 0.10850334 s\n",
      "predicted dataset/test/test1/Tile_0011.png in 0.06939650 s\n",
      "predicted dataset/test/test1/Tile_0012.png in 0.09013867 s\n",
      "predicted dataset/test/test1/Tile_0013.png in 0.05381894 s\n",
      "predicted dataset/test/test1/Tile_0014.png in 0.07375765 s\n",
      "predicted dataset/test/test1/Tile_0015.png in 0.07320166 s\n",
      "predicted dataset/test/test1/Tile_0016.png in 0.07046843 s\n",
      "predicted dataset/test/test1/Tile_0017.png in 0.08716130 s\n",
      "predicted dataset/test/test1/Tile_0018.png in 0.05352068 s\n",
      "predicted dataset/test/test1/Tile_0019.png in 0.05885124 s\n",
      "predicted dataset/test/test1/Tile_0020.png in 0.05721092 s\n",
      "predicted dataset/test/test1/Tile_0021.png in 0.06919479 s\n",
      "predicted dataset/test/test1/Tile_0022.png in 0.06063199 s\n",
      "predicted dataset/test/test1/Tile_0023.png in 0.08080196 s\n",
      "predicted dataset/test/test1/Tile_0024.png in 0.08062291 s\n",
      "predicted dataset/test/test1/Tile_0025.png in 0.06165028 s\n",
      "predicted dataset/test/test1/Tile_0026.png in 0.06555486 s\n",
      "predicted dataset/test/test1/Tile_0027.png in 0.08509827 s\n",
      "predicted dataset/test/test1/Tile_0028.png in 0.08103251 s\n",
      "predicted dataset/test/test1/Tile_0029.png in 0.08800936 s\n",
      "predicted dataset/test/test1/Tile_0030.png in 0.07412362 s\n",
      "predicted dataset/test/test1/Tile_0031.png in 0.06106710 s\n",
      "predicted dataset/test/test1/Tile_0032.png in 0.05901694 s\n",
      "predicted dataset/test/test1/Tile_0033.png in 0.07881498 s\n",
      "predicted dataset/test/test1/Tile_0034.png in 0.07666445 s\n",
      "predicted dataset/test/test1/Tile_0035.png in 0.06464553 s\n",
      "predicted dataset/test/test1/Tile_0036.png in 0.06774926 s\n",
      "predicted dataset/test/test1/Tile_0037.png in 0.07059097 s\n",
      "predicted dataset/test/test1/Tile_0038.png in 0.07335234 s\n",
      "predicted dataset/test/test1/Tile_0039.png in 0.07443953 s\n",
      "predicted dataset/test/test1/Tile_0040.png in 0.09588552 s\n",
      "predicted dataset/test/test1/Tile_0041.png in 0.06311846 s\n",
      "predicted dataset/test/test1/Tile_0042.png in 0.04951024 s\n",
      "predicted dataset/test/test1/Tile_0043.png in 0.04229879 s\n",
      "predicted dataset/test/test1/Tile_0044.png in 0.04853177 s\n",
      "predicted dataset/test/test1/Tile_0045.png in 0.06867886 s\n",
      "predicted dataset/test/test1/Tile_0046.png in 0.06912851 s\n",
      "predicted dataset/test/test1/Tile_0047.png in 0.04731131 s\n",
      "predicted dataset/test/test1/Tile_0048.png in 0.03841281 s\n",
      "predicted dataset/test/test1/Tile_0049.png in 0.05027986 s\n",
      "predicted dataset/test/test1/Tile_0050.png in 0.05946994 s\n",
      "predicted dataset/test/test1/Tile_0051.png in 0.04860616 s\n",
      "predicted dataset/test/test1/Tile_0052.png in 0.08431268 s\n",
      "predicted dataset/test/test1/Tile_0053.png in 0.04499865 s\n",
      "predicted dataset/test/test1/Tile_0054.png in 0.05036664 s\n",
      "predicted dataset/test/test1/Tile_0055.png in 0.04417157 s\n",
      "predicted dataset/test/test1/Tile_0056.png in 0.05548882 s\n",
      "predicted dataset/test/test1/Tile_0057.png in 0.04475570 s\n",
      "predicted dataset/test/test1/Tile_0058.png in 0.04201436 s\n",
      "predicted dataset/test/test1/Tile_0059.png in 0.04284883 s\n",
      "predicted dataset/test/test1/Tile_0060.png in 0.04440331 s\n",
      "predicted dataset/test/test1/Tile_0061.png in 0.04309011 s\n",
      "predicted dataset/test/test1/Tile_0062.png in 0.04357362 s\n",
      "predicted dataset/test/test1/Tile_0063.png in 0.06033015 s\n",
      "predicted dataset/test/test1/Tile_0064.png in 0.06203604 s\n",
      "predicted dataset/test/test1/Tile_0065.png in 0.06167150 s\n",
      "predicted dataset/test/test1/Tile_0066.png in 0.06350255 s\n",
      "predicted dataset/test/test1/Tile_0067.png in 0.04649687 s\n",
      "predicted dataset/test/test1/Tile_0068.png in 0.06221390 s\n",
      "predicted dataset/test/test1/Tile_0069.png in 0.04567051 s\n",
      "predicted dataset/test/test1/Tile_0070.png in 0.04875755 s\n",
      "predicted dataset/test/test1/Tile_0071.png in 0.04445004 s\n",
      "predicted dataset/test/test1/Tile_0072.png in 0.04094148 s\n",
      "predicted dataset/test/test1/Tile_0073.png in 0.05597830 s\n",
      "predicted dataset/test/test1/Tile_0074.png in 0.07994747 s\n",
      "predicted dataset/test/test1/Tile_0075.png in 0.07615376 s\n",
      "predicted dataset/test/test1/Tile_0076.png in 0.06946373 s\n",
      "predicted dataset/test/test1/Tile_0077.png in 0.06029749 s\n",
      "predicted dataset/test/test1/Tile_0078.png in 0.05951214 s\n",
      "predicted dataset/test/test1/Tile_0079.png in 0.06305265 s\n",
      "predicted dataset/test/test1/Tile_0080.png in 0.06410432 s\n",
      "predicted dataset/test/test1/Tile_0081.png in 0.05975103 s\n",
      "predicted dataset/test/test1/Tile_0082.png in 0.09167600 s\n",
      "predicted dataset/test/test1/Tile_0083.png in 0.06869459 s\n",
      "predicted dataset/test/test1/Tile_0084.png in 0.05745053 s\n",
      "predicted dataset/test/test1/Tile_0085.png in 0.05185318 s\n",
      "predicted dataset/test/test1/Tile_0086.png in 0.05585217 s\n",
      "predicted dataset/test/test1/Tile_0087.png in 0.06172538 s\n",
      "predicted dataset/test/test1/Tile_0088.png in 0.04890561 s\n",
      "predicted dataset/test/test1/Tile_0089.png in 0.05967164 s\n",
      "predicted dataset/test/test1/Tile_0090.png in 0.06717086 s\n",
      "predicted dataset/test/test1/Tile_0091.png in 0.05897927 s\n",
      "predicted dataset/test/test1/Tile_0092.png in 0.05505681 s\n",
      "predicted dataset/test/test1/Tile_0093.png in 0.05548477 s\n",
      "predicted dataset/test/test1/Tile_0094.png in 0.05800533 s\n",
      "predicted dataset/test/test1/Tile_0095.png in 0.06558609 s\n",
      "predicted dataset/test/test1/Tile_0096.png in 0.05369568 s\n",
      "predicted dataset/test/test1/Tile_0097.png in 0.07214284 s\n",
      "predicted dataset/test/test1/Tile_0098.png in 0.06753826 s\n",
      "predicted dataset/test/test1/Tile_0099.png in 0.06544971 s\n",
      "predicted dataset/test/test1/Tile_0100.png in 0.06415343 s\n",
      "predicted dataset/test/test1/Tile_0101.png in 0.06074071 s\n",
      "predicted dataset/test/test1/Tile_0102.png in 0.05057836 s\n",
      "predicted dataset/test/test1/Tile_0103.png in 0.06213856 s\n",
      "predicted dataset/test/test1/Tile_0104.png in 0.06421804 s\n",
      "predicted dataset/test/test1/Tile_0105.png in 0.06392264 s\n",
      "predicted dataset/test/test1/Tile_0106.png in 0.08112574 s\n",
      "predicted dataset/test/test1/Tile_0107.png in 0.06223035 s\n",
      "predicted dataset/test/test1/Tile_0108.png in 0.06295633 s\n",
      "predicted dataset/test/test1/Tile_0109.png in 0.06389189 s\n",
      "predicted dataset/test/test1/Tile_0110.png in 0.05125880 s\n",
      "predicted dataset/test/test1/Tile_0111.png in 0.05440927 s\n",
      "predicted dataset/test/test1/Tile_0112.png in 0.06104970 s\n",
      "predicted dataset/test/test1/Tile_0113.png in 0.05342269 s\n",
      "predicted dataset/test/test1/Tile_0114.png in 0.05075073 s\n",
      "predicted dataset/test/test1/Tile_0115.png in 0.05920410 s\n",
      "predicted dataset/test/test1/Tile_0116.png in 0.06086946 s\n",
      "predicted dataset/test/test1/Tile_0117.png in 0.05189967 s\n",
      "predicted dataset/test/test1/Tile_0118.png in 0.05980802 s\n",
      "predicted dataset/test/test1/Tile_0119.png in 0.05467653 s\n",
      "predicted dataset/test/test1/Tile_0120.png in 0.05554008 s\n",
      "predicted dataset/test/test1/Tile_0121.png in 0.04925776 s\n",
      "predicted dataset/test/test1/Tile_0122.png in 0.05215955 s\n",
      "predicted dataset/test/test1/Tile_0123.png in 0.07429910 s\n",
      "predicted dataset/test/test1/Tile_0124.png in 0.05990553 s\n",
      "predicted dataset/test/test1/Tile_0125.png in 0.07048941 s\n",
      "predicted dataset/test/test1/Tile_0126.png in 0.05292082 s\n",
      "predicted dataset/test/test1/Tile_0127.png in 0.05860090 s\n",
      "predicted dataset/test/test1/Tile_0128.png in 0.06499052 s\n",
      "predicted dataset/test/test1/Tile_0129.png in 0.05849552 s\n",
      "predicted dataset/test/test1/Tile_0130.png in 0.06046486 s\n",
      "predicted dataset/test/test1/Tile_0131.png in 0.07866454 s\n",
      "predicted dataset/test/test1/Tile_0132.png in 0.07583785 s\n",
      "predicted dataset/test/test1/Tile_0133.png in 0.06208777 s\n",
      "predicted dataset/test/test1/Tile_0134.png in 0.06898713 s\n",
      "predicted dataset/test/test1/Tile_0135.png in 0.07753611 s\n",
      "predicted dataset/test/test1/Tile_0136.png in 0.07354546 s\n",
      "predicted dataset/test/test1/Tile_0137.png in 0.06364608 s\n",
      "predicted dataset/test/test1/Tile_0138.png in 0.07250786 s\n",
      "predicted dataset/test/test1/Tile_0139.png in 0.06822824 s\n",
      "predicted dataset/test/test1/Tile_0140.png in 0.05877614 s\n",
      "predicted dataset/test/test1/Tile_0141.png in 0.07082558 s\n",
      "predicted dataset/test/test1/Tile_0142.png in 0.06283212 s\n",
      "predicted dataset/test/test1/Tile_0143.png in 0.08203411 s\n",
      "predicted dataset/test/test1/Tile_0144.png in 0.05203724 s\n",
      "predicted dataset/test/test1/Tile_0145.png in 0.06490612 s\n",
      "predicted dataset/test/test1/Tile_0146.png in 0.05594397 s\n",
      "predicted dataset/test/test1/Tile_0147.png in 0.06859303 s\n",
      "predicted dataset/test/test1/Tile_0148.png in 0.05522394 s\n",
      "predicted dataset/test/test1/Tile_0149.png in 0.06649423 s\n",
      "predicted dataset/test/test1/Tile_0150.png in 0.05283594 s\n",
      "predicted dataset/test/test1/Tile_0151.png in 0.06362438 s\n",
      "predicted dataset/test/test1/Tile_0152.png in 0.05379605 s\n",
      "predicted dataset/test/test1/Tile_0153.png in 0.05466151 s\n",
      "predicted dataset/test/test1/Tile_0154.png in 0.05169940 s\n",
      "predicted dataset/test/test1/Tile_0155.png in 0.05089545 s\n",
      "predicted dataset/test/test1/Tile_0156.png in 0.04598498 s\n",
      "predicted dataset/test/test1/Tile_0157.png in 0.05815411 s\n",
      "predicted dataset/test/test1/Tile_0158.png in 0.05259824 s\n",
      "predicted dataset/test/test1/Tile_0159.png in 0.04750896 s\n",
      "predicted dataset/test/test1/Tile_0160.png in 0.04816246 s\n",
      "predicted dataset/test/test1/Tile_0161.png in 0.05016685 s\n",
      "predicted dataset/test/test1/Tile_0162.png in 0.05299163 s\n",
      "predicted dataset/test/test1/Tile_0163.png in 0.05933690 s\n",
      "predicted dataset/test/test1/Tile_0164.png in 0.04868937 s\n",
      "predicted dataset/test/test1/Tile_0165.png in 0.05505490 s\n",
      "predicted dataset/test/test1/Tile_0166.png in 0.05097413 s\n",
      "predicted dataset/test/test1/Tile_0167.png in 0.07049561 s\n",
      "predicted dataset/test/test1/Tile_0168.png in 0.06574368 s\n",
      "predicted dataset/test/test1/Tile_0169.png in 0.05605149 s\n",
      "predicted dataset/test/test1/Tile_0170.png in 0.06033373 s\n",
      "predicted dataset/test/test1/Tile_0171.png in 0.06877470 s\n",
      "predicted dataset/test/test1/Tile_0172.png in 0.06036401 s\n",
      "predicted dataset/test/test1/Tile_0173.png in 0.05659223 s\n",
      "predicted dataset/test/test1/Tile_0174.png in 0.04983854 s\n",
      "predicted dataset/test/test1/Tile_0175.png in 0.07280564 s\n",
      "predicted dataset/test/test1/Tile_0176.png in 0.25760221 s\n",
      "predicted dataset/test/test1/Tile_0177.png in 0.05624795 s\n",
      "predicted dataset/test/test1/Tile_0178.png in 0.05163026 s\n",
      "predicted dataset/test/test1/Tile_0179.png in 0.05378270 s\n",
      "predicted dataset/test/test1/Tile_0180.png in 0.06846428 s\n",
      "predicted dataset/test/test1/Tile_0181.png in 0.10550642 s\n",
      "predicted dataset/test/test1/Tile_0182.png in 0.10522842 s\n",
      "predicted dataset/test/test1/Tile_0183.png in 0.05324769 s\n",
      "predicted dataset/test/test1/Tile_0184.png in 0.05589700 s\n",
      "predicted dataset/test/test1/Tile_0185.png in 0.08548427 s\n",
      "predicted dataset/test/test1/Tile_0186.png in 0.06320333 s\n",
      "predicted dataset/test/test1/Tile_0187.png in 0.06530285 s\n",
      "predicted dataset/test/test1/Tile_0188.png in 0.05660653 s\n",
      "predicted dataset/test/test1/Tile_0189.png in 0.05265498 s\n",
      "predicted dataset/test/test1/Tile_0190.png in 0.05411673 s\n",
      "predicted dataset/test/test1/Tile_0191.png in 0.05237746 s\n",
      "predicted dataset/test/test1/Tile_0192.png in 0.05780768 s\n",
      "predicted dataset/test/test1/Tile_0193.png in 0.06273627 s\n",
      "predicted dataset/test/test1/Tile_0194.png in 0.04709744 s\n",
      "predicted dataset/test/test1/Tile_0195.png in 0.04816151 s\n",
      "predicted dataset/test/test1/Tile_0196.png in 0.04642582 s\n",
      "predicted dataset/test/test1/Tile_0197.png in 0.04669714 s\n",
      "predicted dataset/test/test1/Tile_0198.png in 0.04796004 s\n",
      "predicted dataset/test/test1/Tile_0199.png in 0.04075193 s\n",
      "predicted dataset/test/test1/Tile_0200.png in 0.03937507 s\n",
      "predicted dataset/test/test1/Tile_0201.png in 0.04083109 s\n",
      "predicted dataset/test/test1/Tile_0202.png in 0.06083202 s\n",
      "predicted dataset/test/test1/Tile_0203.png in 0.04847860 s\n",
      "predicted dataset/test/test1/Tile_0204.png in 0.05990267 s\n",
      "predicted dataset/test/test1/Tile_0205.png in 0.04165721 s\n",
      "predicted dataset/test/test1/Tile_0206.png in 0.04103231 s\n",
      "predicted dataset/test/test1/Tile_0207.png in 0.04402471 s\n",
      "predicted dataset/test/test1/Tile_0208.png in 0.03792381 s\n",
      "predicted dataset/test/test1/Tile_0209.png in 0.04507828 s\n",
      "predicted dataset/test/test1/Tile_0210.png in 0.05256557 s\n",
      "predicted dataset/test/test1/Tile_0211.png in 0.06483626 s\n",
      "predicted dataset/test/test1/Tile_0212.png in 0.03984785 s\n",
      "predicted dataset/test/test1/Tile_0213.png in 0.04444027 s\n",
      "predicted dataset/test/test1/Tile_0214.png in 0.04109621 s\n",
      "predicted dataset/test/test1/Tile_0215.png in 0.04237199 s\n",
      "predicted dataset/test/test1/Tile_0216.png in 0.06004930 s\n",
      "predicted dataset/test/test1/Tile_0217.png in 0.05032754 s\n",
      "predicted dataset/test/test1/Tile_0218.png in 0.04569077 s\n",
      "predicted dataset/test/test1/Tile_0219.png in 0.04874754 s\n",
      "predicted dataset/test/test1/Tile_0220.png in 0.04537010 s\n",
      "predicted dataset/test/test1/Tile_0221.png in 0.06185269 s\n",
      "predicted dataset/test/test1/Tile_0222.png in 0.04340720 s\n",
      "predicted dataset/test/test1/Tile_0223.png in 0.04397392 s\n",
      "predicted dataset/test/test1/Tile_0224.png in 0.04413700 s\n",
      "predicted dataset/test/test1/Tile_0225.png in 0.06015944 s\n",
      "predicted dataset/test/test1/Tile_0226.png in 0.05236101 s\n",
      "predicted dataset/test/test1/Tile_0227.png in 0.04696345 s\n",
      "predicted dataset/test/test1/Tile_0228.png in 0.07792187 s\n",
      "predicted dataset/test/test1/Tile_0229.png in 0.04401088 s\n",
      "predicted dataset/test/test1/Tile_0230.png in 0.04406428 s\n",
      "predicted dataset/test/test1/Tile_0231.png in 0.04328537 s\n",
      "predicted dataset/test/test1/Tile_0232.png in 0.04724264 s\n",
      "predicted dataset/test/test1/Tile_0233.png in 0.05625153 s\n",
      "predicted dataset/test/test1/Tile_0234.png in 0.04404688 s\n",
      "predicted dataset/test/test1/Tile_0235.png in 0.04426599 s\n",
      "predicted dataset/test/test1/Tile_0236.png in 0.05090904 s\n",
      "predicted dataset/test/test1/Tile_0237.png in 0.04761314 s\n",
      "predicted dataset/test/test1/Tile_0238.png in 0.05200291 s\n",
      "predicted dataset/test/test1/Tile_0239.png in 0.04490733 s\n",
      "predicted dataset/test/test1/Tile_0240.png in 0.04600906 s\n",
      "predicted dataset/test/test1/Tile_0241.png in 0.04527617 s\n",
      "predicted dataset/test/test1/Tile_0242.png in 0.04868889 s\n",
      "predicted dataset/test/test1/Tile_0243.png in 0.05681705 s\n",
      "predicted dataset/test/test1/Tile_0244.png in 0.05380225 s\n",
      "predicted dataset/test/test1/Tile_0245.png in 0.04767394 s\n",
      "predicted dataset/test/test1/Tile_0246.png in 0.05703497 s\n",
      "predicted dataset/test/test1/Tile_0247.png in 0.04683137 s\n",
      "predicted dataset/test/test1/Tile_0248.png in 0.05005646 s\n",
      "predicted dataset/test/test1/Tile_0249.png in 0.04869151 s\n",
      "predicted dataset/test/test1/Tile_0250.png in 0.04834032 s\n",
      "predicted dataset/test/test1/Tile_0251.png in 0.04707527 s\n",
      "predicted dataset/test/test1/Tile_0252.png in 0.05045581 s\n",
      "predicted dataset/test/test1/Tile_0253.png in 0.04484963 s\n",
      "predicted dataset/test/test1/Tile_0254.png in 0.05038404 s\n",
      "predicted dataset/test/test1/Tile_0255.png in 0.04620409 s\n",
      "predicted dataset/test/test1/Tile_0256.png in 0.04632306 s\n",
      "predicted dataset/test/test1/Tile_0257.png in 0.04463768 s\n",
      "predicted dataset/test/test1/Tile_0258.png in 0.05561185 s\n",
      "predicted dataset/test/test1/Tile_0259.png in 0.04371619 s\n",
      "predicted dataset/test/test1/Tile_0260.png in 0.05692053 s\n",
      "predicted dataset/test/test1/Tile_0261.png in 0.04310465 s\n",
      "predicted dataset/test/test1/Tile_0262.png in 0.05030656 s\n",
      "predicted dataset/test/test1/Tile_0263.png in 0.04370618 s\n",
      "predicted dataset/test/test1/Tile_0264.png in 0.06466413 s\n",
      "predicted dataset/test/test1/Tile_0265.png in 0.04524326 s\n",
      "predicted dataset/test/test1/Tile_0266.png in 0.04738331 s\n",
      "predicted dataset/test/test1/Tile_0267.png in 0.04389286 s\n",
      "predicted dataset/test/test1/Tile_0268.png in 0.05241323 s\n",
      "predicted dataset/test/test1/Tile_0269.png in 0.04457831 s\n",
      "predicted dataset/test/test1/Tile_0270.png in 0.04959059 s\n",
      "predicted dataset/test/test1/Tile_0271.png in 0.05305171 s\n",
      "predicted dataset/test/test1/Tile_0272.png in 0.05250955 s\n",
      "predicted dataset/test/test1/Tile_0273.png in 0.04855847 s\n",
      "predicted dataset/test/test1/Tile_0274.png in 0.04616451 s\n",
      "predicted dataset/test/test1/Tile_0275.png in 0.04454398 s\n",
      "predicted dataset/test/test1/Tile_0276.png in 0.04906654 s\n",
      "predicted dataset/test/test1/Tile_0277.png in 0.04585981 s\n",
      "predicted dataset/test/test1/Tile_0278.png in 0.04856706 s\n",
      "predicted dataset/test/test1/Tile_0279.png in 0.04909062 s\n",
      "predicted dataset/test/test1/Tile_0280.png in 0.05023336 s\n",
      "predicted dataset/test/test1/Tile_0281.png in 0.04328227 s\n",
      "predicted dataset/test/test1/Tile_0282.png in 0.05223680 s\n",
      "predicted dataset/test/test1/Tile_0283.png in 0.04460359 s\n",
      "predicted dataset/test/test1/Tile_0284.png in 0.05487633 s\n",
      "predicted dataset/test/test1/Tile_0285.png in 0.04368496 s\n",
      "predicted dataset/test/test1/Tile_0286.png in 0.04641008 s\n",
      "predicted dataset/test/test1/Tile_0287.png in 0.04417372 s\n",
      "predicted dataset/test/test1/Tile_0288.png in 0.05867648 s\n",
      "predicted dataset/test/test1/Tile_0289.png in 0.04723287 s\n",
      "predicted dataset/test/test1/Tile_0290.png in 0.04978228 s\n",
      "predicted dataset/test/test1/Tile_0291.png in 0.04380774 s\n",
      "predicted dataset/test/test1/Tile_0292.png in 0.06396890 s\n",
      "predicted dataset/test/test1/Tile_0293.png in 0.04642129 s\n",
      "predicted dataset/test/test1/Tile_0294.png in 0.04727483 s\n",
      "predicted dataset/test/test1/Tile_0295.png in 0.04504871 s\n",
      "predicted dataset/test/test1/Tile_0296.png in 0.05070758 s\n",
      "predicted dataset/test/test1/Tile_0297.png in 0.05360532 s\n",
      "predicted dataset/test/test1/Tile_0298.png in 0.04904246 s\n",
      "predicted dataset/test/test1/Tile_0299.png in 0.04806566 s\n",
      "predicted dataset/test/test1/Tile_0300.png in 0.05290246 s\n",
      "predicted dataset/test/test1/Tile_0301.png in 0.05081439 s\n",
      "predicted dataset/test/test1/Tile_0302.png in 0.04460573 s\n",
      "predicted dataset/test/test1/Tile_0303.png in 0.04642963 s\n",
      "predicted dataset/test/test1/Tile_0304.png in 0.04960704 s\n",
      "predicted dataset/test/test1/Tile_0305.png in 0.04495883 s\n",
      "predicted dataset/test/test1/Tile_0306.png in 0.05129004 s\n",
      "predicted dataset/test/test1/Tile_0307.png in 0.04497504 s\n",
      "predicted dataset/test/test1/Tile_0308.png in 0.04582739 s\n",
      "predicted dataset/test/test1/Tile_0309.png in 0.04703259 s\n",
      "predicted dataset/test/test1/Tile_0310.png in 0.05573463 s\n",
      "predicted dataset/test/test1/Tile_0311.png in 0.04583335 s\n",
      "predicted dataset/test/test1/Tile_0312.png in 0.05180120 s\n",
      "predicted dataset/test/test1/Tile_0313.png in 0.04433179 s\n",
      "predicted dataset/test/test1/Tile_0314.png in 0.05145884 s\n",
      "predicted dataset/test/test1/Tile_0315.png in 0.04374552 s\n",
      "predicted dataset/test/test1/Tile_0316.png in 0.06721783 s\n",
      "predicted dataset/test/test1/Tile_0317.png in 0.04563904 s\n",
      "predicted dataset/test/test1/Tile_0318.png in 0.04560399 s\n",
      "predicted dataset/test/test1/Tile_0319.png in 0.04539156 s\n",
      "predicted dataset/test/test1/Tile_0320.png in 0.04837942 s\n",
      "predicted dataset/test/test1/Tile_0321.png in 0.04741406 s\n",
      "predicted dataset/test/test1/Tile_0322.png in 0.06234026 s\n",
      "predicted dataset/test/test1/Tile_0323.png in 0.05123281 s\n",
      "predicted dataset/test/test1/Tile_0324.png in 0.05122018 s\n",
      "predicted dataset/test/test1/Tile_0325.png in 0.04716563 s\n",
      "predicted dataset/test/test1/Tile_0326.png in 0.04570699 s\n",
      "predicted dataset/test/test1/Tile_0327.png in 0.13174081 s\n",
      "predicted dataset/test/test1/Tile_0328.png in 0.04770994 s\n",
      "predicted dataset/test/test1/Tile_0329.png in 0.04534984 s\n",
      "predicted dataset/test/test1/Tile_0330.png in 0.04054379 s\n",
      "predicted dataset/test/test1/Tile_0331.png in 0.03756189 s\n",
      "predicted dataset/test/test1/Tile_0332.png in 0.04564047 s\n",
      "predicted dataset/test/test1/Tile_0333.png in 0.03988767 s\n",
      "predicted dataset/test/test1/Tile_0334.png in 0.03852916 s\n",
      "predicted dataset/test/test1/Tile_0335.png in 0.03974152 s\n",
      "predicted dataset/test/test1/Tile_0336.png in 0.03641129 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "from tiled_detiled import tiled_image\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "sensitivty = 0.5\n",
    "target_f = \"dataset/raw/labeled/10m/DJI_0007.jpg\"\n",
    "\n",
    "if len(os.listdir('dataset/test/test1')) != 0:\n",
    "    print(\"Found previous tiled images, attemp delete\")\n",
    "    file_to_delete = os.listdir('dataset/test/test1')\n",
    "    for f in file_to_delete:\n",
    "        os.remove('dataset/test/test1/' + f)\n",
    "\n",
    "target_im = cv2.imread(target_f)\n",
    "#save otuput\n",
    "from glob import glob\n",
    "import math\n",
    "result_heigth = target_im.shape[0]\n",
    "result_width = target_im.shape[1]\n",
    "tile_size = 256\n",
    "\n",
    "if target_im is not None:\n",
    "    tiled_image(target_im, 256, \"dataset/test/test1\")\n",
    "else:\n",
    "    print(\"Cannot read ifle\")\n",
    "\n",
    "\n",
    "#Slice image\n",
    "predict_file = sorted(glob(\"dataset/test/test1/*.png\"))\n",
    "predict_file = iter(predict_file)\n",
    "# print(predict_file)\n",
    "\n",
    "predicted_list = []\n",
    "\n",
    "#remove existing file\n",
    "if len(os.listdir('dataset/test/result_test1')) != 0:\n",
    "    print(\"Found previous predicted files, attemp delete\")\n",
    "    file_to_delete = os.listdir('dataset/test/result_test1')\n",
    "    for f in file_to_delete:\n",
    "        os.remove('dataset/test/result_test1/' + f)\n",
    "#predict slided image\n",
    "nn = 1\n",
    "for f in predict_file:\n",
    "    target = f\n",
    "    # print(target)\n",
    "    inputs = cv2.imread(target)\n",
    "    tic = time.time()\n",
    "    r = predict_tile(inputs, sensitivity)\n",
    "    toc = time.time()\n",
    "    print(\"predicted %s in %.8f s\" %(f, toc - tic))\n",
    "    tf.keras.preprocessing.image.save_img('dataset/test/result_test1/result_%s.png'% str(nn).zfill(4),r)\n",
    "    nn += 1\n",
    "\n",
    "\n",
    "\n",
    "file = sorted(glob(\"dataset/test/result_test1/*\"))\n",
    "\n",
    "temp = np.zeros((math.ceil(result_heigth / tile_size) * tile_size, math.ceil(result_width / tile_size) * tile_size,3))\n",
    "\n",
    "i = 0\n",
    "for r in range(0, temp.shape[0], tile_size):\n",
    "    for c in range(0, temp.shape[1], tile_size):\n",
    "        tile = cv2.imread(file[i])\n",
    "        temp[r:r + tile_size, c:c + tile_size,:] = tile \n",
    "        i += 1\n",
    "\n",
    "cv2.imwrite(\"predicted_location.jpg\" , temp[:result_heigth, :5274,:])\n",
    "# cv2.imwrite(\"predicted_location_DJI_20211208095147_0040.JPG\" , temp[:5460, :8192,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3956, 5274, 3) (3956, 5274, 3)\n",
      "IoU socre is:  0.17567833865594257\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def convert_bw(img):\n",
    "    grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    bw = cv2.threshold(grayImage, 128, 255, cv2.THRESH_BINARY)[1]\n",
    "    image_bw = Image.fromarray(bw)\n",
    "    return image_bw\n",
    "\n",
    "_ground_truth = cv2.imread('DJI_0007_mask.jpg')\n",
    "_pred = cv2.imread(\"predicted_location.jpg\")\n",
    "gt = convert_bw(_ground_truth)\n",
    "pd = convert_bw(_pred)\n",
    "\n",
    "print(_ground_truth.shape, _pred.shape)\n",
    "intersection = np.logical_and(gt, pd)\n",
    "union = np.logical_or(gt, pd)\n",
    "iou_score = np.sum(intersection) / np.sum(union)\n",
    "print(\"IoU socre is: \", iou_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8fcb75110a54a95449da3edd6bc4968834d7697970658473e67a56856224e4f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('cv_asagau': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
